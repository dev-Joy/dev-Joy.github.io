{"posts":[{"title":"Apache Spark 설치","text":"Window 운영체제1. Anaconda 설치하기Anaconda Prompt에서 python 3.8 이상인지 확인 where python 으로 Python이 어디에 설치되어 있는지 확인 pip : python package installerwhere pip으로 pip이 어디에 있는지 확인 pip --version으로 pip 버전 확인 2. JAVAjava --version으로 버전 확인Extended Support Until이 긴 JAVA 8(LTS)을 다운받는다 3. Spark 다운받은 파일을 원하는 위치에 이동한다. 4. Hadoophadoop-2.7.7을 사용다운받은 파일을 원하는 위치에 이동한다. 5. PysparkAnaconda Prompt에서 pip install pyspark 환경변수 설정주의사항: 파일경로에 빈칸이 존재하면 Error가 발생 JAVA_HOMED:\\ProgramFiles\\Java\\java-14-openjdk-14.0.2.12\\bin HADOOP_HOMED:\\ProgramFiles\\Hadoop SPARK_HOMED:\\ProgramFiles\\Spark\\spark-3.2.0-bin-hadoop2.7 PYSPARK_PYTHONwhere python을 해서 D:\\ProgramFiles\\Anaconda3\\python.exe 환경변수 &gt; 변수: Path를 선택하여 [편집]버튼을 누르고 아래와 같이 추가 Mac 운영체제1. Anaconda 설치하기2. which python 으로 경로 찾기3. Homebrew 설치1234567891011121314#JAVA 버전확인java -version# JAVA8 설치brew install --cask adoptopenjdk8# SCALA 설치brew install scala# Apache Spark 설치brew install apache-spark# pip 버전확인pip --version# pyspark 설치pip install pyspark# pyspark 터미널 뜨는지 확인pyspark","link":"/2022/11/03/Apache-Spark-%EC%84%A4%EC%B9%98/"},{"title":"Google Machine Learning Bootcamp 2022 수료후기","text":"출처: Gather Town에서 나홀로 사진찍기 [출처: Google Developers 국문 블로그]2022년 3기 구글 머신러닝 부트캠프프로그램 기간: 2022.06.22 ~ 2022.09.19(약 3개월) Mission1: Coursera 강의 수료-앤드류 응(Andrew Ng) 교수님의 딥러닝 특화과정(Deep Learning Specialization) 코스를 수강하였습니다. Coursera 강의를 수강하다보면 가끔씩 한글자막이 이상하거나 없는 경우가 있었습니다.DeepLearningAI Youtube 또는 boostcourse의 한글자막을 이용할 때도 있었습니다. Course1: Neural Networks and Deep Learning Course2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization Course3: Structuring Machine Learning Projects Course4: Convolutional Neural Networks Course5: Sequence Models강의 들으면서 머신러닝에 대해 전반적인 기초지식을 배웠습니다. 수료증 TIP: 모든 Course를 다 듣고 최종 Course 수료증 받으시려면 신분증 인증하셔야 합니다. Mission2: Kaggle Tabular playground series 제출 or Competition 상위 25% 매 달마다 Kaggle Tabular playground series가 열립니다.빅데이터 분석기사 실기 공부하면서 배운 지식으로 TPS를 제출하였습니다.참고 사이트: DataMamin , Big Data Certification KR참고 서적: 위키독스, 더북, 파이썬 머신러닝 완벽 가이드, 핸즈온 머신러닝, 파이썬 라이브러리를 활용한 머신러닝 Mission3: Certification 취득 Tensorflow Certification GCP Professional Data Engineer certification GCP Professional ML Engineer certification 위와 같이 3가지 자격증 중 하나를 취득해야하는 데Laurence Moroney의 TensorFlow Developer Professional Certificate 강의 제공을 지원하고Tensorflow에 대해 공부하고 싶어서 Tensorflow Certification을 선택하였습니다.TF강의 Github 저는 Colab에서 무료GPU 환경에서 문제를 풀어 model을 만든 뒤 h5 파일을 download 해서 시험에 제출하였습니다. 123456789101112131415161718192021import tensorflow as tffrom tensorflow.python.keras.callbacks import ModelCheckpointfrom google.colab import files### 시험을 치룰 때 유용한 코드 checkpoint_path = 'mymodel_checkpoint.ckpt' # 학습중인 모델 자동으로 저장 checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only= True, save_best_only=True,# 검증 세트 손실이 기존의 최고 성능 모델에 비해 낮아질때만 저장 monitor='val_loss', verbose=1) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(X_train, y_train, validation_data=(X_test, y_test, callbacks=[checkpoint], epochs=10) model.load_weights(checkpoint_path) # model 가중치 저장 model = solution_model() model.save(&quot;mymodel.h5&quot;) files.download(&quot;mymodel.h5&quot;) # model 파일 다운로드 Tensorflow 튜토리얼 &gt; Tensorflow 시험접수 &gt; Tensorflow 시험 가이드 &gt; Tensorflow 시험환경 Setting하기 UDACITY Tensorflow 자격증 강의 Intro to TensorFlow for Deep Learning 무료 강의Udacity Class Materials Notion출처: 소소한블로그 응시료 : $100 (해외 결제가 가능한 카드) 시험 시간 : 300분 시험 유형 : Category 1: Basic / Simple model Category 2: Model from learning dataset Category 3: Convolutional Neural Network with real-world image dataset Category 4: NLP Text Classification with real-world text dataset Category 5: Sequence Model with real-world numeric dataset출처 : 테디노트 시험 환경 : 인터넷이 연결되어 있는 어디에서나 가능 IDE : PyCharm에 시험용 Plugin(TensorFlow Developer Certificate) 설치 ( Communiy 버전도 가능) 시험 준비물 : 여권이나 영문 운전면허증 또는 주민등록증, 셀카 재응시 기간1번째 불합격 시, 14일 이후 재응시 가능2번째 불합격 시, 2개월 이후 재응시 가능3번째 불합격 시, 1년 이후 재응시 가능 합격점수 : 총 100점 만점에 90점 이상 결과발표 : 응시자가 시험 제출 시 채점 과정을 거쳐 응시자 포털에 점수 게시. 시험 합격 후에는 등록된 이메일 주소로 인증서가 발송(영업일 기준 10일 이내) 인증서 유효기간 : 36개월Google Developers Certification에서 합격자를 볼 수 있습니다. Tech Talk &amp; Office Hour &amp; Resume Clinic &amp; Mentoring 등 [출처: Google Developers 국문 블로그] 매주 진행되는 회사별 Tech Talk, Office Hour을 하면서현업에서 어떤 Machine Learning을 사용하는지 설명해주셨습니다.또한 사전질문을 하여 좋은 답변을 얻을 수 있었고 Office Tour가 진행되는 회사도 있었습니다.Resume Clinic, Mentoring을 통해서 이력서를 어떻게 작성하고 어떤 방향으로 준비해야할지 알았습니다. Google Machine Learning Goods미션을 수행하는데 힘이 되준 Gifts잘쓰겠습니다~ 너무 예뻐요 😍질문하면 친절하게 답변주신 구글 머신러닝 부트캠프 순선님, 누리님과 참가자분들 모두들 감사합니다! Gift Gift Gift Gift Google Machine Learning Bootcamp 시작하기전에 하면 좋았을 점 Python 자료구조 및 알고리즘 공부추천강의: 한국외대 신찬수 교수님 Youtube 코딩테스트 준비추천사이트: 백준, 프로그래머스, LeetCode 영어 공부AMA(Ask Me Anything) session with Kevin Murphy을 하였을 때 영어공부를 더 열심히 해야 겠다고 생각했습니다.영어를 잘했더라면 질문도 했을텐데…ㅠㅠ 말씀하시는 속도도 빨라서 순간 멍때리면 알아듣지 못하는 경우도 있었다😢 한줄평 Google Machine Learning Bootcamp 수료가 끝이 아니라 Machine Learning 시작이다🏃‍♀️🏃‍♂️🏃","link":"/2022/09/24/Google-Machine-Learning-Bootcamp-2022-%EC%88%98%EB%A3%8C%ED%9B%84%EA%B8%B0/"},{"title":"Tools 소개","text":"계속 업데이트 됩니다! MobaXtermPutty + FTP DBeaverDB Editor cmderWindow에서 Linux 명령어 사용가능 obsidianmarkdown 노트 MarpMarkdown Presentation Ecosystem","link":"/2023/02/08/Tools-%EC%86%8C%EA%B0%9C/"},{"title":"Tableau Embedding","text":"Tableau Server에서 설정 Tableau Server에 신뢰할 수 있는 IP 주소 또는 호스트 이름 추가 Tableau Embedding API Version 2 Tableau Rest API Java Github Tableau Embedding API Version 3 Tableau의 연결된 앱을 구성하여 내장된 콘텐츠에 대한 SSO 지원 tsm(Tableau Server Manager)12345678910111213141516171819202122cd /opt/tableau/tableau_server/tsm configuration get -k gateway.public.host#server.tableau.comtsm configuration get -k gateway.trusted#127.0.0.1, 127.0.0.2tsm configuration get -k gateway.trusted_hosts#server.tableau.comtsm configuration get -k gateway.public.port#443tsm configuration get -k vizportal.rest_api.cors.enabled#truetsm configuration get -k vizportal.rest_api.cors.allow_origin#https://127.0.0.1 https://server.tableau.com, https://127.0.0.2, https://web.tableau.comtsm configuration get -k wgserver.unrestricted_ticket#truetsm configuration get -k wgserver.clickjack_defense.enabled#falsetsm configuration get -k vizportal.oauth.external_authorization_server.max_expiration_period_in_minutes#600tsm pending-changes apply Window hosts 파일 우클릭 &gt; Code(으)로 열기 &gt; 수정 &gt; 저장 &gt; “Failed to save ‘hosts’: Insufficient permissions. Select ‘Retry as Admin’ to retry as administrator.”라는 알림창이 뜨면 [Retry as Admin…] 클릭 &gt; Windows 명령 처리기 [예] 클릭 C:\\Windows\\System32\\drivers\\etc\\hosts12127.0.0.1 server.tableau.com127.0.0.2 web.tableau.com MacMac(iterm, terminal)123456sudo vim /private/etc/hosts #i127.0.0.1 server.tableau.com127.0.0.2 web.tableau.com# :wq *.tableau.com로 Domain 맞춰서 SameSite Error 해결 Python, Java, JavaScript로 구현 token이 제대로 됐는지 https://jwt.io/에서 확인 JSONWebToken.py1234567891011121314151617import jwttoken = jwt.encode( { &quot;iss&quot;: connectedAppClientId, &quot;exp&quot;: datetime.datetime.utcnow() + datetime.timedelta(minutes=5), &quot;jti&quot;: str(uuid.uuid4()), &quot;aud&quot;: &quot;tableau&quot;, &quot;sub&quot;: user, &quot;scp&quot;: [&quot;tableau:views:embed&quot;, &quot;tableau:metrics:embed&quot;] }, connectedAppSecretKey, algorithm = &quot;HS256&quot;, headers = { 'kid': connectedAppSecretId, 'iss': connectedAppClientId } ) JSONWebToken.java12345678910111213141516171819202122232425262728import com.nimbusds.jose.*;import com.nimbusds.jose.crypto.*;import com.nimbusds.jwt.*;import java.util.*;...String secret = &quot;secretvalue&quot;; String kid = &quot;connectedAppSecretId&quot;; String clientId = &quot;connectedAppClientId&quot;; List&lt;String&gt; scopes = newArrayList&lt;&gt;(Arrays.asList(&quot;tableau:views:embed&quot;)); String username = &quot;username&quot;; JWSSigner signer = new MACSigner(secret); JWSHeader header = newJWSHeader.Builder(JWSAlgorithm.HS256).keyID(kid).customParam(&quot;iss&quot;, clientId).build(); JWTClaimsSet claimsSet = new JWTClaimsSet.Builder() .issuer(clientId) .expirationTime(new Date(new Date().getTime() + 60 * 1000)) //expires in 1 minute .jwtID(UUID.randomUUID().toString()) .audience(&quot;tableau&quot;) .subject(username) .claim(&quot;scp&quot;, scopes) .build(); SignedJWT signedJWT = new SignedJWT(header, claimsSet); signedJWT.sign(signer); model.addAttribute(&quot;token&quot;, signedJWT.serialize()); nest.js123456789101112131415161718192021222324252627282930313233343536import { Injectable } from '@nestjs/common';import { JwtService } from '@nestjs/jwt';import { v4 as uuid } from 'uuid';import * as config from 'config';const tableauConfig = config.get('tableau');@Injectable()export class TableauService { constructor(private jwtService: JwtService) {} async getTableauToken(username: string){ const token = await this.jwtService.sign( { iss: tableauConfig.connectedAppClientId, aud: 'tableau', jti: uuid(), sub: username, scp: [ 'tableau:view:embed', 'tableau:views:embed_authoring', ], }, { header: { alg: 'HS256', kid: tableauConfig.connectedAppSecretId, iss: tableauConfig.connectedAppClientId, }, secret: tableauConfig.secretvalue, expiresIn: '5m', }, ); return token; }}","link":"/2023/03/01/Tableau-Embedding/"},{"title":"신찬수-자료구조와알고리즘","text":"한국외대 신찬수 교수님 자료구조와알고리즘 자료구조(Data Structure), 알고리즘(Algoritm)자료: data -&gt; [저장공간(memory) + 읽기,쓰기,삽입,삭제,탐색(연산)] =&gt; 구조알고리즘: data (유한한 횟수의 연산들) 입력-&gt; 정답 출력 자료구조 (예:) 1. 변수(variable) 2.배열(array), 리스트(list)알고리즘 (예:) 100개의 정수: 리스트 A:입력 -&gt; 오름차순 정렬:출력 explnation.py123456789101112a = 5 # 쓰기 연산print(a) # 읽기 연산A = [3, -1, 5, 7]'''접근: 원소의 index읽기, 쓰기: A[3]삽입: A.append(9) # 맨 끝에 추가, A.insert(0, 100) # 0번째 idx에 100 값 추가삭제: A.pop() # 가장 마지막 제거, A.pop(2) # 2번쨰 idx 값 제거''' 인류 최초의 알고리즘 ac, 페르시아, Algebra 수학자 Al-khwarizmi-&gt; Algorismus + Arithmos =&gt; [Algorithm] 최대공약수(GCD)계산 알고리즘 by Euclidgreatest_common_divisor.py12345def gcd(a, b): while a!=0 and b!=0: if a&gt;b: a = a-b else: b = b-a return a+b","link":"/2022/11/20/%EC%8B%A0%EC%B0%AC%EC%88%98-%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0%EC%99%80%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/08/28/hello-world/"},{"title":"big-O","text":"date: 2022-09-20 23:26:03Big O notationBig Ω notation/ Big θ notationBig O notation 시간 복잡도(점근적 실행 시간, asymptotic runtime) O(big-O): 시간의 상한배열의 모든 값을 출력하는 알고리즘은 O(N)으로 표현할 수 있지만O(N^3),O(N^2),O(2^N)도 옳은 표현이다.다시 말하자면 알고리즘의 수행 시간은 적어도 이들 중 하나보다 빠르기만 하면 된다. Ω (big-Omega): 시간의 하한배열의 모든 값을 출력하는 알고리즘은 Ω(N) 뿐만 아니라 Ω(logN) 혹은 Ω(1)로도 표현할 수 있다.결국, 해당 알고리즘은 Ω 수행 시간보다 빠를 수 없게 된다. θ (big-theta): 딱 맞는 수행 시간θ는 O와 Ω 둘 다 의미한다. 즉, 어떤 알고리즘의 수행시간이 O(N)이면서 Ω(N)이라면, 이 알고리즘의 수행 시간을 θ(N)로 표현할 수 있다. 공간 복잡도 사용하는 스택 공간 또는 공간 복잡도 계산에 포함된다. 함수들이 호출 스택에 동시에 존재하지 않으면 계산에 포함되지 않는다. 상수항은 무시하라O(N) = O(2N) 지배적이지 않은 항은 무시하라O(N^2) = O(N^2 + N) 여러 부분으로 이루어진 알고리즘: 덧셈 vs 곱셈 O(A+B)만약 알고리즘이 “A 일을 모두 끝마친 후에 B일을 수행하라”의 형태라면 A와 B의 수행 시간을 더해야 한다. O(A*B)만약 알고리즘이 “A 일을 할 떄마다 B 일을 수행하라”의 형태라면 A와 B의 수행시간을 곱해야 한다.","link":"/2022/09/20/big-O/"},{"title":"CSV 파일 다루기","text":"Header 즉 첫번째 줄 linux에서 삭제하기12345678910111213141516171819202122232425# 덮어쓰기 전의 파일을 확장자.bak를 붙여서 보관하는 것이 가능하다sed -i.bak -e '/^#/d' config.txt# DATA.csv에서 첫번째 줄 제거하여 DATA_NO_HEADER.csv 생성sed '1d' DATA.csv &gt; DATA_NO_HEADER.csv# 동일한 이름으로 첫번째 줄 제거sed -i '1d' DATA.csv# 패턴을 포함하는 선을 삭제vi DATA.csv:g/&lt;pattern&gt;/d:wq# '#'문자가 포함된 라인 삭제하면서 공백 제거된 내용만 출력sed '/#/d' jupyterhub_config.py | sed '/^$/d'#덮어쓰기cat &gt; 명령어는 덮어쓰기 명령어이다.# 이어쓰기. cat &gt;&gt; [파일 경로 / 이름] cat [ 파일 경로 / 이름 ] 파일을 열어 내용을 출력 cat &gt; [ 파일 경로 / 이름 ] 같은 이름의 파일이 없는 경우 : 파일 새로 만들고 내용 입력하기 같은 이름의 파일이 있는 경우 : 파일 덮어쓰고 내용 새로 입력하기 cat &gt;&gt; [ 파일 경로 / 이름 ] 같은 이름의 파일이 없는 경우 : 파일 새고 만들고 내용 입력하기 같은 이름의 파일이 있는 경우 : 기존 파일의 내용 밑에 이어쓰기 첫줄 일괄 삭제해서find . -type f -name “*.csv” -exec sed -i “-new” ‘1d’ {} ; CSV Comma Separated Values HBase와 Hive 차이는? HBase는 NoSQL 데이터베이스이고 Hive는 하둡잡을 실행하는 DW 프레임워크이다. HBase는 HDFS위에서 동작하고, Hive는 MapReduce 위에서 동작한다. Schema flexibility: 다양한 구조와 포맷의 데이터를 처리하고 저장합니다. SQL-like queries: Hadoop에서 Data management: Hive DDL하이브는 CSV 형식의 파일을 효과적으로 적재하기 위한 CSV 서데를 제공한다. Hive 0.14 버전부터 기본 지원 CSV 서데를 이용하면 테이블 칼럼의 타입은 String 으로 고정 sepratorChar: 칼럼간의 구분자 quoteChar: 칼럼의 값을 지정한 문자로 묶어준다. escapeChar: 칼럼에 데이터를 입력할 때 파싱하지 않고 무시 12345678910111213CREATE TABLE my_table( a string, b string)ROW FORMAT SERDE'org.apache.hadoop.hive.serde2.OpenCSVSerde'WITH SERDEPROPERTIES( &quot;separatorChar&quot; = &quot;,&quot;, &quot;quoteChar&quot; = &quot;'&quot;, &quot;escapeChar&quot; = &quot;\\\\&quot;)STORED AS TEXTFILE; [참고한 게시글]https://118k.tistory.com/451","link":"/2024/01/05/CSV-%ED%8C%8C%EC%9D%BC-%EB%8B%A4%EB%A3%A8%EA%B8%B0/"},{"title":"Array","text":"Time Complexity: O(N)Space Complexity: O(N)값을 index로 접근 Python1234567A.append, A.pop # O(1) 평균A.insert, A.remove # O(n) Worst CaseA.index, A.count # O(n) Worst CaseA.pop() # 평균 O(1)A.pop(2) # O(n) Worst Case# 평균 O(1) Hashtable list 리스트: 용량 자동조절(dynamic array) 12345import sysA = [] # 빈 리스트print(sys.getsizeof(A)) # 28bytesA.append(10) # A = [10]print(sys.getsizeof(A)) # 44bytes list class:capacity: 용량n: 현재 저장된 값의 개수 append구현12345678910111213A.append(X):if A.n &lt; A.capacity: A[n] = x A.n = n + 1else: A.n == A.capacityB = A.capacity*2 #A.capacity*2 크기의 리스트 새로 할당for i in range(n): B[i] = A[i] # O(n)del AA = BA[n] = xA.n = n+1 A[0] = 2를 넣으면A[0]의 주소가 2가 저장되어있는 주소를 가르킴A[0]+1을 하면 A[0] = 3이 됨그러면 3이 저장되어있는 새로운 주소를 가르킴 1234567891011121314list.append(값)append: 맨 뒤에 값 추가list.pop()pop: 맨뒤의 값을 지우고 리턴pop(index): A[index]를 제거하고 리턴list.insert(index, value)list[index]에 value를 삽입list.remove(value): list에서 value제거list.index(value), list.count(value) JavaScript push, pop, unshift, shift concat indexOf, lastIndexOf, includes -join-split -splice: 배열자체를 변형-slice(start, end) : end exclusive하다 Array 배열 map, forEach, filter, find, findIndex, reduce, every, some 등 내장 iteration 메소드를 활용한다. map : array → array forEach: array 한개씩 순회하여 콜백 호출 filter: 필터링 find: 한개 찾아 반환 findIndex: 한개 찾아 인덱스 반환 reduce: array → single value reduceRight: 배열거꾸로 부터 누적 every: 모두 만족하면 true 값을 반환 some: 한개라도 만족하면 true 값을 반환","link":"/2024/07/03/Array/"},{"title":"Nginx Reverse Proxy 설정","text":"JAVA 파일을 빌드하여 Root.war 파일을 생성합니다. terminal1234# bootWar./gradlew bootWar# War./gradlew war Tomcat 설정 apache-tomcat-10.1.28/webapps/ 에 ROOT.war를 넣어줍니다. apache-tomcat-10.1.28/bin 폴더로 이동하여 setenv.sh 파일을 만듭니다. 환경 변수를 설정합니다. setenv.sh123456789101112131415161718192021222324252627282930export JAVA_OPTS=&quot;$JAVA_OPTS -Dspring.profiles.active=local&quot;export CATALINA_OPTS=&quot;$CATALINA_OPTS -Dspring.application.batch.option=true&quot;# 환경 변수 설정export SECRET_KEY=&quot;abcdefghijklmnop&quot;export DATABASENAME=&quot;postgres&quot;export USERNAME=&quot;postgres&quot;export PASSWORD=&quot;postgres&quot;# spring.datasource 관련 환경 변수 설정export DB_HOST=&quot;127.0.0.1&quot;export DB_PORT=&quot;5432&quot;# Spring Boot 애플리케이션에서 환경 변수를 참조하도록 설정export JAVA_OPTS=&quot;$JAVA_OPTS -Dspring.config.import=classpath:application-common.properties&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Dspring.datasource.url=jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DATABASENAME}&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Dspring.datasource.username=${USERNAME}&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Dspring.datasource.password=${PASSWORD}&quot;# Hibernate 설정export JAVA_OPTS=&quot;$JAVA_OPTS -Dspring.jpa.hibernate.ddl-auto=update&quot;# Main Settingexport JAVA_OPTS=&quot;$JAVA_OPTS -Ddataportal.mainsetting.batchenabled=true&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Ddataportal.mainsetting.website-dbms=POSTGRESQL&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Ddataportal.mainsetting.website-host=${DB_HOST}&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Ddataportal.mainsetting.website-port=${DB_PORT}&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Ddataportal.mainsetting.website-database-name=website&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Ddataportal.mainsetting.website-user-name=admin&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Ddataportal.mainsetting.website-password=admin&quot; apache-tomcat-10.1.28/bin 폴더로 이동하여 catalina.sh 파일을 vim 또는 editor로 엽니다. JAVA_HOME 설정을 해줍니다. (Tomcat 서버가 실행될 때 필요한 JAVA와 관련한 위치를 연결하기 위해서) catalina.sh12345# Oracle JAVA 설치시 echo $PATH# 또는# brew로 JAVA 설치시/usr/libexec/java_home 참고: mac JAVA_HOME 환경변수 설정 catalina.sh파일에 아래를 추가해줍니다. 12345# OS specific support. $var _must_ be set to either true or false.# Set JAVA_HOME and update PATHexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Homeexport PATH=$JAVA_HOME/bin:$PATH Tomcat을 실행합니다. tomcat 실행시 Root.war가 자동으로 풀립니다. 12cd apache-tomcat-10.1.28/bin./startup.sh Nginx와 같은 웹 서버를 왜 사용할까요? 클라이언트의 요청 처리를 분산시킬 수 있는 로드 밸런스를 사용함으로써 효율적인 처리를 합니다. 정적파일을 다이렉트로 제공해주기 때문에 백엔드 서버에 부담을 주지 않습니다. 클라이언트는 Ngnix 포트로만 백엔드 서버에 접근할 수 있어 보안에 도움이 됩니다. 개념설명리버스 프록시란 클라이언트 요청을 대신 받아 내부 서버로 전달해주는 것을 리버스 프록시(Reverse Proxy) 라고 합니다. 프록시란 대리라는 의미로, 정보를 대신 전달해주는 주체라고 생각하면 되는데, 만약 이 프록시 없이 웹 서버를 운영한다고 가정합니다. 사용자 앞단에 있으면 Forward Proxy 서버 앞단에 있으면 Reverse Proxy localhost:3000 라고 하는 웹서버를 열어서 운영했을 때, 사용자가 갑자기 많아지거나, 웹서버가 그대로 노출되어 있기 때문에 보안적으로 위험성이 있겠죠? nginx를 사용하면 로드 밸런싱으로 부하를 줄여줄 수 있고, 분산 처리 또한 가능하며 웹서버의 SSL 인증도 적용할 수 있습니다. 따라서 아래와 같이 사용자 -&gt; nginx -&gt; 웹서버로 구성해서 사용자의 요청을 nginx가 대신 웹서버로 전달해주도록 구성합니다. Reverse Proxy 사용이유 동시성 : 다중 연결을 더 잘 처리할 수 있는 역방향 프록시를 추가하면 백엔드 서버 성능이 크게 향상될 수 있다. 복원력 : 백엔드 서버의 상태를 모니터링하고 서비스가 재개될 때까지 실패한 서버에 대한 요청 전송을 중지할 수 있다. 역방향 프록시가 여전히 작동 중인 백엔드 서버에 요청을 자동으로 보내기 때문에 클라이언트에는 오류가 표시되지 않는다. 확장성 : 역방향 프록시는 백엔드 서버 그룹에 대한 단일 “공개 얼굴”이기 때문에 변화하는 트래픽 부하에 따라 서버를 추가 및 제거할 수 있다. 레이어 7라우팅 : 역방향 프록시는 모든 서버로 향하는 트래픽을 보고 필요에 따라 요청과 응답을 수정하여 각 요청을 보낼 위치에 대한 지능적인 결정을 내릴 수 있다. 요청의 특정 HTTP 헤더, URL의 일부, 클라이언트의 지리적 위치 등에 따라 라우팅 결정을 내릴 수 있다. 캐싱 : 역방향 프록시는 캐싱을 수행하기에 좋은 장소이다. 일반적으로 모든 요청을 백엔드 서버로 보내고 각 백엔드 서버가 자체 캐시를 구축하도록 하는 것보다 콘텐츠를 개시하는 것이 훨씬 더 효율적이다. 기타 기능 : 역방향 프록시는 백엔드 서버 앞에 앉음으로써 대역폭 또는 요청 속도에 따른 트래픽 형성, 연결 제한, 다양한 인증 체계와의 통합, 활동 모니터링 등과 같은 다른 기능도 수행할 수 있다. 포트 프론트엔드(Web) : 3000 port (React Local Port) 백엔드(Server, API): 8080 port (Tomcat Default Port) Ngix reverse proxy 구성 출처: https://velog.io/@chickenfondue","link":"/2024/11/05/Nginx/"}],"tags":[{"name":"Data Engineering","slug":"Data-Engineering","link":"/tags/Data-Engineering/"},{"name":"Apache Spark","slug":"Apache-Spark","link":"/tags/Apache-Spark/"},{"name":"Google Machine Learning Bootcamp 2022","slug":"Google-Machine-Learning-Bootcamp-2022","link":"/tags/Google-Machine-Learning-Bootcamp-2022/"},{"name":"tools","slug":"tools","link":"/tags/tools/"},{"name":"Tableau Embedding","slug":"Tableau-Embedding","link":"/tags/Tableau-Embedding/"},{"name":"algorithms","slug":"algorithms","link":"/tags/algorithms/"},{"name":"data structures","slug":"data-structures","link":"/tags/data-structures/"},{"name":"Data Engineering,Hive","slug":"Data-Engineering-Hive","link":"/tags/Data-Engineering-Hive/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"}],"categories":[{"name":"Data Engineering","slug":"Data-Engineering","link":"/categories/Data-Engineering/"},{"name":"Tableau","slug":"Tableau","link":"/categories/Tableau/"},{"name":"review","slug":"review","link":"/categories/review/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"},{"name":"Apache Spark","slug":"Data-Engineering/Apache-Spark","link":"/categories/Data-Engineering/Apache-Spark/"},{"name":"data structures","slug":"data-structures","link":"/categories/data-structures/"},{"name":"algorithms","slug":"algorithms","link":"/categories/algorithms/"},{"name":"Tableau Embedding","slug":"Tableau/Tableau-Embedding","link":"/categories/Tableau/Tableau-Embedding/"},{"name":"Hadoop Ecosystem","slug":"Data-Engineering/Hadoop-Ecosystem","link":"/categories/Data-Engineering/Hadoop-Ecosystem/"},{"name":"server","slug":"server","link":"/categories/server/"}],"pages":[{"title":"about","text":"OSMU(One source Multi Use) 사용자 중심의 사고를 가지고 데이터 프로덕트를 만들기 위해 노력합니다. Study &amp; Learning Javascript, Typescript, React, Next.js Git, GitHub, Docker Python, SQL, Tableau","link":"/about/index.html"}]}